{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS330_Homework2_Stencil.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkoC8rAYBE7"
      },
      "source": [
        "\n",
        "##Setup\n",
        "\n",
        "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkP5aBdfFkd"
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
        "if not os.path.isdir('./omniglot_resized'):\n",
        "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
        "                                        dest_path='./omniglot_resized.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "assert os.path.isdir('./omniglot_resized')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4A-uBwf8OeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e9eff9-f65d-4998-bcb2-00f9dbec684d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtiYUiwI-1K"
      },
      "source": [
        "\"\"\" Utility functions. \"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "## Loss utilities\n",
        "def cross_entropy_loss(pred, label, k_shot):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_dTnU8JwWWc"
      },
      "source": [
        "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "seed = 123\n",
        "def conv_block(inp, cweight, bweight, bn, activation=tf.nn.relu, residual=False, training=True):\n",
        "  \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
        "  stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
        "\n",
        "  conv_output = tf.nn.conv2d(input=inp, filters=cweight, strides=no_stride, padding='SAME') + bweight\n",
        "  normed = bn(conv_output)\n",
        "  normed = activation(normed)\n",
        "  normed = tf.nn.max_pool(normed, stride, stride, padding='VALID')\n",
        "  return normed\n",
        "\n",
        "class ConvLayers(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
        "    super(ConvLayers, self).__init__()\n",
        "    self.channels = channels\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.dim_output = dim_output\n",
        "    self.img_size = img_size\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    dtype = tf.float32\n",
        "    weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
        "    k = 3\n",
        "\n",
        "    weights['conv1'] = tf.Variable(weight_initializer(shape=[k, k, self.channels, self.dim_hidden]), name='conv1', dtype=dtype)\n",
        "    weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b1')\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization(name='bn1')\n",
        "    weights['conv2'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv2', dtype=dtype)\n",
        "    weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b2')\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization(name='bn2')\n",
        "    weights['conv3'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv3', dtype=dtype)\n",
        "    weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b3')\n",
        "    self.bn3 = tf.keras.layers.BatchNormalization(name='bn3')\n",
        "    weights['conv4'] = tf.Variable(weight_initializer([k, k, self.dim_hidden, self.dim_hidden]), name='conv4', dtype=dtype)\n",
        "    weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b4')\n",
        "    self.bn4 = tf.keras.layers.BatchNormalization(name='bn4')\n",
        "    weights['w5'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_output]), name='w5', dtype=dtype)\n",
        "    weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
        "    self.conv_weights = weights\n",
        "\n",
        "  def call(self, inp, weights, training=True):\n",
        "    channels = self.channels\n",
        "    inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
        "    hidden1 = conv_block(inp, weights['conv1'], weights['b1'], self.bn1, training = training)\n",
        "    hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], self.bn2, training = training)\n",
        "    hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], self.bn3, training = training)\n",
        "    hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], self.bn4, training = training)\n",
        "    hidden4 = tf.reduce_mean(input_tensor=hidden4, axis=[1, 2])\n",
        "    return tf.matmul(hidden4, weights['w5']) + weights['b5']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXZS_JULriBh"
      },
      "source": [
        "\"\"\"Data loading scripts\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "import imageio\n",
        "\n",
        "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "    paths: A list of character folders\n",
        "    labels: List or numpy array of same length as paths\n",
        "    n_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "    List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if n_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, n_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "           for i, path in zip(labels, paths)\n",
        "           for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "    filename: Image filename\n",
        "    dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "    1 channel image\n",
        "  \"\"\"\n",
        "  image = imageio.imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      num_classes: Number of classes for classification (K-way)\n",
        "      num_samples_per_class: num samples to generate per class in one batch\n",
        "      num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
        "      num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
        "      batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "    self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
        "    self.num_meta_test_classes = num_meta_test_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "               for family in os.listdir(data_folder)\n",
        "               if os.path.isdir(os.path.join(data_folder, family))\n",
        "               for character in os.listdir(os.path.join(data_folder, family))\n",
        "               if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(123)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "      num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "      batch_type: meta_train/meta_val/meta_test\n",
        "      shuffle: randomly shuffle classes or not\n",
        "      swap: swap number of classes (N) and number of samples per class (K) or not\n",
        "    Returns:\n",
        "      A a tuple of (1) Image batch and (2) Label batch where\n",
        "      image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
        "      where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"meta_train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    elif batch_type == \"meta_val\":\n",
        "      folders = self.metaval_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "      num_classes = self.num_meta_test_classes\n",
        "      num_samples_per_class = self.num_meta_test_samples_per_class\n",
        "    all_image_batches, all_label_batches = [], []\n",
        "    for i in range(batch_size):\n",
        "      sampled_character_folders = random.sample(\n",
        "        folders, num_classes)\n",
        "      labels_and_images = get_images(sampled_character_folders, range(\n",
        "        num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
        "      labels = [li[0] for li in labels_and_images]\n",
        "      images = [image_file_to_array(\n",
        "        li[1], self.dim_input) for li in labels_and_images]\n",
        "      images = np.stack(images)\n",
        "      labels = np.array(labels).astype(np.int32)\n",
        "      labels = np.reshape(\n",
        "        labels, (num_classes, num_samples_per_class))\n",
        "      labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
        "      images = np.reshape(\n",
        "        images, (num_classes, num_samples_per_class, -1))\n",
        "\n",
        "      batch = np.concatenate([labels, images], 2)\n",
        "      \n",
        "      if shuffle:\n",
        "        for p in range(num_samples_per_class):\n",
        "          np.random.shuffle(batch[:, p])\n",
        "\n",
        "      labels = batch[:, :, :num_classes]\n",
        "      images = batch[:, :, num_classes:]\n",
        "\n",
        "      if swap:\n",
        "        labels = np.swapaxes(labels, 0, 1)\n",
        "        images = np.swapaxes(images, 0, 1)\n",
        "\n",
        "      all_image_batches.append(images)\n",
        "      all_label_batches.append(labels)\n",
        "    all_image_batches = np.stack(all_image_batches)\n",
        "    all_label_batches = np.stack(all_label_batches)\n",
        "    return all_image_batches, all_label_batches"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxriXFvwsGfp"
      },
      "source": [
        "\"\"\"MAML model code\"\"\"\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "  def __init__(self, dim_input=1, dim_output=1,\n",
        "               num_inner_updates=1,\n",
        "               inner_update_lr=0.4, num_filters=32, k_shot=5, learn_inner_update_lr=False):\n",
        "    super(MAML, self).__init__()\n",
        "    self.dim_input = dim_input\n",
        "    self.dim_output = dim_output\n",
        "    self.inner_update_lr = inner_update_lr\n",
        "    self.loss_func = partial(cross_entropy_loss, k_shot=k_shot)\n",
        "    self.dim_hidden = num_filters\n",
        "    self.channels = 1\n",
        "    self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
        "\n",
        "    # outputs_ts[i] and losses_ts_post[i] are the output and loss after i+1 inner gradient updates\n",
        "    losses_tr_pre, outputs_tr, losses_ts_post, outputs_ts = [], [], [], []\n",
        "    accuracies_tr_pre, accuracies_ts = [], []\n",
        "\n",
        "    # for each loop in the inner training loop\n",
        "    outputs_ts = [[]]*num_inner_updates\n",
        "    losses_ts_post = [[]]*num_inner_updates\n",
        "    accuracies_ts = [[]]*num_inner_updates\n",
        "\n",
        "    # Define the weights - these should NOT be directly modified by the\n",
        "    # inner training loop\n",
        "    tf.random.set_seed(seed)\n",
        "    self.conv_layers = ConvLayers(self.channels, self.dim_hidden, self.dim_output, self.img_size)\n",
        "\n",
        "    self.learn_inner_update_lr = learn_inner_update_lr\n",
        "    if self.learn_inner_update_lr:\n",
        "      self.inner_update_lr_dict = {}\n",
        "      for key in self.conv_layers.conv_weights.keys():\n",
        "        self.inner_update_lr_dict[key] = [tf.Variable(self.inner_update_lr, name='inner_update_lr_%s_%d' % (key, j)) for j in range(num_inner_updates)]\n",
        "  \n",
        "\n",
        "  def call(self, inp, meta_batch_size=25, num_inner_updates=1):\n",
        "    def task_inner_loop(inp, reuse=True,\n",
        "                      meta_batch_size=25, num_inner_updates=1):\n",
        "      \"\"\"\n",
        "        Perform gradient descent for one task in the meta-batch (i.e. inner-loop).\n",
        "        Args:\n",
        "          inp: a tuple (input_tr, input_ts, label_tr, label_ts), where input_tr and label_tr are the inputs and\n",
        "            labels used for calculating inner loop gradients and input_ts and label_ts are the inputs and\n",
        "            labels used for evaluating the model after inner updates.\n",
        "            Should be shapes:\n",
        "              input_tr: [N*K, 784]\n",
        "              input_ts: [N*K, 784]\n",
        "              label_tr: [N*K, N]\n",
        "              label_ts: [N*K, N]\n",
        "        Returns:\n",
        "          task_output: a list of outputs, losses and accuracies at each inner update\n",
        "      \"\"\"\n",
        "      # the inner and outer loop data\n",
        "      input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "      # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
        "      weights = self.conv_layers.conv_weights\n",
        "\n",
        "      # the predicted outputs, loss values, and accuracy for the pre-update model (with the initial weights)\n",
        "      # evaluated on the inner loop training data\n",
        "      task_output_tr_pre, task_loss_tr_pre, task_accuracy_tr_pre = None, None, None\n",
        "\n",
        "      # lists to keep track of outputs, losses, and accuracies of test data for each inner_update\n",
        "      # where task_outputs_ts[i], task_losses_ts[i], task_accuracies_ts[i] are the output, loss, and accuracy\n",
        "      # after i+1 inner gradient updates\n",
        "      task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
        "  \n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "      # perform num_inner_updates to get modified weights\n",
        "      # modified weights should be used to evaluate performance\n",
        "      # Note that at each inner update, always use input_tr and label_tr for calculating gradients\n",
        "      # and use input_ts and labels for evaluating performance\n",
        "\n",
        "      # HINTS: You will need to use tf.GradientTape().\n",
        "      # Read through the tf.GradientTape() documentation to see how 'persistent' should be set.\n",
        "      # Here is some documentation that may be useful: \n",
        "      # https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
        "      # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "\n",
        "      with tf.GradientTape(persistent=True) as tape1:\n",
        "        task_output_tr_pre = self.conv_layers(input_tr, weights)\n",
        "        task_loss_tr_pre = self.loss_func(pred = task_output_tr_pre, label=label_tr)\n",
        "\n",
        "      grads = tape1.gradient(task_loss_tr_pre, list(weights.values()))\n",
        "      gradients = dict(zip(weights.keys(), grads))\n",
        "      fast_weights = dict(zip(weights.keys(), [weights[key] - self.inner_update_lr*gradients[key] for key in weights.keys()]))\n",
        "        \n",
        "      for i in range(num_inner_updates-1):\n",
        "\n",
        "        test_output = self.conv_layers(input_ts, fast_weights, training=False)\n",
        "        task_outputs_ts.append(tf.stop_gradient(test_output))\n",
        "        ts_loss = self.loss_func(pred = tf.stop_gradient(test_output), label=label_ts)\n",
        "        task_losses_ts.append(ts_loss)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape2:\n",
        "          preds = self.conv_layers(input_tr, fast_weights)\n",
        "          loss  = self.loss_func(pred = preds, label=label_tr)\n",
        "\n",
        "        grads = tape2.gradient(loss, list(fast_weights.values()))\n",
        "        gradients = dict(zip(fast_weights.keys(), grads))\n",
        "        fast_weights = dict(zip(fast_weights.keys(), [fast_weights[key] - self.inner_update_lr*gradients[key] for key in fast_weights.keys()]))\n",
        "\n",
        "      test_output = self.conv_layers(input_ts, fast_weights, training=False)\n",
        "      task_outputs_ts.append(test_output)\n",
        "      ts_loss = self.loss_func(pred = test_output, label=label_ts)\n",
        "      task_losses_ts.append(ts_loss)\n",
        "\n",
        "      #############################\n",
        "\n",
        "      # Compute accuracies from output predictions\n",
        "      task_accuracy_tr_pre = accuracy(tf.argmax(input=label_tr, axis=1), tf.argmax(input=tf.nn.softmax(task_output_tr_pre), axis=1))\n",
        "\n",
        "      for j in range(num_inner_updates):\n",
        "        task_accuracies_ts.append(accuracy(tf.argmax(input=label_ts, axis=1), tf.argmax(input=tf.nn.softmax(task_outputs_ts[j]), axis=1)))\n",
        "\n",
        "      task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
        "\n",
        "      return task_output\n",
        "\n",
        "    input_tr, input_ts, label_tr, label_ts = inp\n",
        "    # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
        "    unused = task_inner_loop((input_tr[0], input_ts[0], label_tr[0], label_ts[0]),\n",
        "                          False,\n",
        "                          meta_batch_size,\n",
        "                          num_inner_updates)\n",
        "    out_dtype = [tf.float32, [tf.float32]*num_inner_updates, tf.float32, [tf.float32]*num_inner_updates]\n",
        "    out_dtype.extend([tf.float32, [tf.float32]*num_inner_updates])\n",
        "    task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "    result = tf.map_fn(task_inner_loop_partial,\n",
        "                    elems=(input_tr, input_ts, label_tr, label_ts),\n",
        "                    dtype=out_dtype,\n",
        "                    parallel_iterations=meta_batch_size)\n",
        "    return result\n",
        "   "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1pz_ousUsz"
      },
      "source": [
        "\"\"\"Model training code\"\"\"\n",
        "\"\"\"\n",
        "Usage Instructions:\n",
        "  5-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=25 --k_shot=1 --inner_update_lr=0.4 --num_inner_updates=1 --logdir=logs/omniglot5way/\n",
        "  20-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=16 --k_shot=1 --n_way=20 --inner_update_lr=0.1 --num_inner_updates=5 --logdir=logs/omniglot20way/\n",
        "  To run evaluation, use the '--meta_train=False' flag and the '--meta_test_set=True' flag to use the meta-test set.\n",
        "\"\"\"\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from collections import OrderedDict\n",
        "\n",
        "@tf.function\n",
        "def outer_train_step(inp, model, optim, meta_batch_size=25, num_inner_updates=1):\n",
        "  with tf.GradientTape(persistent=False) as outer_tape:\n",
        "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts\n",
        "\n",
        "@tf.function\n",
        "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
        "  result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "  outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts  \n",
        "\n",
        "\n",
        "def meta_train_fn(model, exp_string, data_generator,\n",
        "               n_way=5, meta_train_iterations=15000, meta_batch_size=25,\n",
        "               log=True, logdir='/tmp/data', k_shot=1, num_inner_updates=1, meta_lr=0.001):\n",
        "  SUMMARY_INTERVAL = 10\n",
        "  SAVE_INTERVAL = 100\n",
        "  PRINT_INTERVAL = 10\n",
        "  TEST_PRINT_INTERVAL = PRINT_INTERVAL *5\n",
        "\n",
        "  pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
        "\n",
        "  meta_val_accs = OrderedDict()\n",
        "\n",
        "  for itr in range(meta_train_iterations):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of training data and partition into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "\n",
        "    image_batches, label_batches = data_generator.sample_batch('meta_train', meta_batch_size)\n",
        "    input_tr = image_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    label_tr = label_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    input_ts = image_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    label_ts = label_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    #############################\n",
        "\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    \n",
        "    result = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    if (itr+1) % SUMMARY_INTERVAL == 0:\n",
        "      pre_accuracies.append(result[-2])\n",
        "      post_accuracies.append(result[-1][-1])\n",
        "\n",
        "    if (itr!=0) and (itr+1) % PRINT_INTERVAL == 0:\n",
        "      print_str = 'Iteration %d: pre-inner-loop train accuracy: %.5f, post-inner-loop test accuracy: %.5f' % (itr+1, np.mean(pre_accuracies), np.mean(post_accuracies))\n",
        "      print(print_str)\n",
        "      pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "    if (itr!=0) and (itr+1) % TEST_PRINT_INTERVAL == 0:\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "\n",
        "      # sample a batch of validation data and partition it into\n",
        "      # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "      # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "      image_batches, label_batches = data_generator.sample_batch('meta_val', meta_batch_size)\n",
        "      input_tr = image_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "      label_tr = label_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "      input_ts = image_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "      label_ts = label_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "\n",
        "      #############################\n",
        "\n",
        "      inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "      result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "      print('Meta-validation pre-inner-loop train accuracy: %.5f, meta-validation post-inner-loop test accuracy: %.5f' % (result[-2], result[-1][-1]))\n",
        "\n",
        "      meta_val_accs[itr+1] = result[-1][-1].numpy()\n",
        "\n",
        "  model_file = logdir + exp_string +  '/model' + str(itr)\n",
        "  print(\"Saving to \", model_file)\n",
        "  model.save_weights(model_file)\n",
        "\n",
        "  log_file = logdir + 'meta_train' + exp_string + '.pkl'\n",
        "\n",
        "  with open(log_file, 'wb') as f:\n",
        "    pickle.dump(meta_val_accs, f)\n",
        "\n",
        "# calculated for omniglot\n",
        "NUM_META_TEST_POINTS = 600\n",
        "\n",
        "def meta_test_fn(model, data_generator, exp_string, n_way=5, meta_batch_size=25, k_shot=1,\n",
        "              num_inner_updates=1):\n",
        "  \n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "\n",
        "  meta_test_accuracies = []\n",
        "\n",
        "  for _ in range(NUM_META_TEST_POINTS):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    \n",
        "    image_batches, label_batches = data_generator.sample_batch('meta_test', meta_batch_size)\n",
        "    input_tr = image_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    label_tr = label_batches[:, :, :k_shot, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    input_ts = image_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    label_ts = label_batches[:, :, k_shot:, :].reshape(meta_batch_size, n_way*k_shot, -1)\n",
        "    \n",
        "    #############################\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    meta_test_accuracies.append(result[-1][-1])\n",
        "\n",
        "  meta_test_accuracies = np.array(meta_test_accuracies)\n",
        "  means = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  ci95 = 1.96*stds/np.sqrt(NUM_META_TEST_POINTS)\n",
        "\n",
        "  print('Mean meta-test accuracy/loss, stddev, and confidence intervals')\n",
        "  print((means, stds, ci95))\n",
        "\n",
        "  meta_test_results = {'meta_test_acc': means, 'meta_test_acc_std': stds,\n",
        "                       'CI95': ci95, 'num_meta_test_points': NUM_META_TEST_POINTS}\n",
        "\n",
        "  log_file = logdir + 'meta_test' + exp_string + '.pkl'\n",
        "\n",
        "  with open(log_file, 'wb') as f:\n",
        "    pickle.dump(meta_test_results, f)\n",
        "\n",
        "def run_maml(n_way=5, k_shot=1, meta_batch_size=25, meta_lr=0.001,\n",
        "             inner_update_lr=0.4, num_filters=32, num_inner_updates=1,\n",
        "             learn_inner_update_lr=False,\n",
        "             resume=False, resume_itr=0, log=True, logdir='/tmp/data/',\n",
        "             data_path='./omniglot_resized',meta_train=True,\n",
        "             meta_train_iterations=500, meta_train_k_shot=-1,\n",
        "             meta_train_inner_update_lr=-1):\n",
        "\n",
        "\n",
        "  # call data_generator and get data with k_shot*2 samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
        "\n",
        "  # set up MAML model\n",
        "  dim_output = data_generator.dim_output\n",
        "  dim_input = data_generator.dim_input\n",
        "  model = MAML(dim_input,\n",
        "              dim_output,\n",
        "              num_inner_updates=num_inner_updates,\n",
        "              inner_update_lr=inner_update_lr,\n",
        "              k_shot=k_shot,\n",
        "              num_filters=num_filters,\n",
        "              learn_inner_update_lr=learn_inner_update_lr)\n",
        "\n",
        "  if meta_train_k_shot == -1:\n",
        "    meta_train_k_shot = k_shot\n",
        "  if meta_train_inner_update_lr == -1:\n",
        "    meta_train_inner_update_lr = inner_update_lr\n",
        "\n",
        "  exp_string = '.n_way_'+str(n_way)+'.mbs_'+str(meta_batch_size) + '.k_shot_' + str(meta_train_k_shot) + '.inner_numstep_' + str(num_inner_updates) + '.inner_updatelr_' + str(meta_train_inner_update_lr) + '.learn_inner_update_lr_' + str(learn_inner_update_lr)\n",
        "\n",
        "\n",
        "  if meta_train:\n",
        "    meta_train_fn(model, exp_string, data_generator,\n",
        "                  n_way, meta_train_iterations, meta_batch_size, log, logdir,\n",
        "                  k_shot, num_inner_updates, meta_lr)\n",
        "    \n",
        "    print(\"Evaluating trained MAML on held-out test-classes\")\n",
        "    \n",
        "    meta_test_fn(model, data_generator, exp_string, n_way=n_way,\n",
        "                 meta_batch_size=1,k_shot=k_shot,\n",
        "                 num_inner_updates=num_inner_updates)\n",
        "    \n",
        "  else:\n",
        "    meta_batch_size = 1\n",
        "\n",
        "    model_file = tf.train.latest_checkpoint(logdir + exp_string)\n",
        "    print(\"Restoring model weights from \", model_file)\n",
        "    model.load_weights(model_file)\n",
        "    \n",
        "    meta_test_fn(model, data_generator, exp_string, n_way, meta_batch_size, k_shot, num_inner_updates)\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USOh7VulTMK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17fa60b-890f-4410-fdad-836813b2af73"
      },
      "source": [
        "logdir = '/content/drive/MyDrive/CS330-MetaLearning/Hw2/maml/'\n",
        "run_maml(n_way=5, k_shot=1, inner_update_lr= 0.4, num_inner_updates=1, logdir=logdir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "Iteration 10: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.24000\n",
            "Iteration 20: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.22400\n",
            "Iteration 30: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.25600\n",
            "Iteration 40: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.24800\n",
            "Iteration 50: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.25600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.24800\n",
            "Iteration 60: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.29600\n",
            "Iteration 70: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.30400\n",
            "Iteration 80: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 90: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 100: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.29600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.32800\n",
            "Iteration 110: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 120: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 130: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.32000\n",
            "Iteration 140: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.36800\n",
            "Iteration 150: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.40800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.24000, meta-validation post-inner-loop test accuracy: 0.33600\n",
            "Iteration 160: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 170: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 180: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.34400\n",
            "Iteration 190: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 200: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.37600\n",
            "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.35200\n",
            "Iteration 210: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.33600\n",
            "Iteration 220: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.35200\n",
            "Iteration 230: pre-inner-loop train accuracy: 0.28800, post-inner-loop test accuracy: 0.37600\n",
            "Iteration 240: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 250: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.40000\n",
            "Meta-validation pre-inner-loop train accuracy: 0.27200, meta-validation post-inner-loop test accuracy: 0.40800\n",
            "Iteration 260: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 270: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.36000\n",
            "Iteration 280: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.38400\n",
            "Iteration 290: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 300: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.43200\n",
            "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 310: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 320: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 330: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.44800\n",
            "Iteration 340: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 350: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.36800\n",
            "Iteration 360: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.43200\n",
            "Iteration 370: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.42400\n",
            "Iteration 380: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
            "Iteration 390: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 400: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.44800\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.43200\n",
            "Iteration 410: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 420: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.45600\n",
            "Iteration 430: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 440: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.46400\n",
            "Iteration 450: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.50400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.52800\n",
            "Iteration 460: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.50400\n",
            "Iteration 470: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.47200\n",
            "Iteration 480: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.56000\n",
            "Iteration 490: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.51200\n",
            "Iteration 500: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.54400\n",
            "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.44800\n",
            "Saving to  /content/drive/MyDrive/CS330-MetaLearning/Hw2/maml/.n_way_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model499\n",
            "Evaluating trained MAML on held-out test-classes\n",
            "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
            "(0.50033337, 0.1952773, 0.015625438559352615)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CG3QYmKyqZ",
        "outputId": "492124f6-3f79-43b0-e24b-de78c425f4fa"
      },
      "source": [
        "log_file = logdir + 'meta_train.n_way_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False' + '.pkl'\n",
        "with open(log_file, 'rb') as f:\n",
        "  logs = pickle.load(f)\n",
        "\n",
        "print(dict(logs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{50: 0.248, 100: 0.32799998, 150: 0.336, 200: 0.352, 250: 0.40800002, 300: 0.36800003, 350: 0.36800003, 400: 0.432, 450: 0.52800006, 500: 0.44800004}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmGfgV-geFj"
      },
      "source": [
        "# models/ProtoNet\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class ProtoNet(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_filters, latent_dim):\n",
        "    super(ProtoNet, self).__init__()\n",
        "    self.num_filters = num_filters\n",
        "    self.latent_dim = latent_dim\n",
        "    num_filter_list = self.num_filters + [latent_dim]\n",
        "    self.convs = []\n",
        "    for i, num_filter in enumerate(num_filter_list):\n",
        "      block_parts = [\n",
        "        layers.Conv2D(\n",
        "          filters=num_filter,\n",
        "          kernel_size=3,\n",
        "          padding='SAME',\n",
        "          activation='linear'),\n",
        "      ]\n",
        "\n",
        "      block_parts += [layers.BatchNormalization()]\n",
        "      block_parts += [layers.Activation('relu')]\n",
        "      block_parts += [layers.MaxPool2D()]\n",
        "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
        "      self.__setattr__(\"conv%d\" % i, block)\n",
        "      self.convs.append(block)\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "  def call(self, inp):\n",
        "    out = inp\n",
        "    for conv in self.convs:\n",
        "      out = conv(out)\n",
        "    out = self.flatten(out)\n",
        "    return out\n",
        "\n",
        "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
        "  \"\"\"\n",
        "    calculates the prototype network loss using the latent representation of x\n",
        "    and the latent representation of the query set\n",
        "    Args:\n",
        "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
        "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
        "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
        "      num_classes: number of classes (N) for classification\n",
        "      num_support: number of examples (S) in the support set\n",
        "      num_queries: number of examples (Q) in the query set\n",
        "    Returns:\n",
        "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
        "      acc: the accuracy of classification on the queries\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "\n",
        "  # compute the prototypes\n",
        "  # compute the distance from the prototypes\n",
        "  # compute cross entropy loss\n",
        "  # note - additional steps are needed!\n",
        "  # return the cross-entropy loss and accuracy\n",
        "\n",
        "  prototypes = tf.reduce_mean(tf.reshape(x_latent,[num_classes, num_support, -1]), axis=1)\n",
        "\n",
        "  tiled_proto = tf.tile(tf.expand_dims(prototypes, axis=0), (num_classes*num_queries, 1, 1))\n",
        "  tiled_queries = tf.tile(tf.expand_dims(q_latent, axis=1), (1, num_classes, 1))\n",
        "  \n",
        "  distances = tf.reduce_mean(tf.square(tiled_proto - tiled_queries), axis=2)\n",
        "\n",
        "  log_probs = tf.reshape(tf.nn.log_softmax(-distances), [num_classes, num_queries, -1])\n",
        "  ce_loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(labels_onehot, log_probs), axis=-1), [-1]))\n",
        "\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(log_probs, axis=-1), tf.argmax(labels_onehot, axis=-1)), tf.float32))\n",
        "\n",
        "  #############################\n",
        "  return ce_loss, acc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_bOml4PhkSM"
      },
      "source": [
        "# run_ProtoNet\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@tf.function\n",
        "def proto_net_train_step(model, optim, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    x_latent = model(x)\n",
        "    q_latent = model(q)\n",
        "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return ce_loss, acc\n",
        "\n",
        "@tf.function\n",
        "def proto_net_eval(model, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  x_latent = model(x)\n",
        "  q_latent = model(q)\n",
        "  ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  return ce_loss, acc \n",
        "\n",
        "def run_protonet(data_path='./omniglot_resized', logdir= '/tmp/data/',n_way=20, \n",
        "                 k_shot=1, n_query=5, n_meta_test_way=20, k_meta_test_shot=5, n_meta_test_query=5):\n",
        "  n_epochs = 20\n",
        "  n_episodes = 100\n",
        "\n",
        "  im_width, im_height, channels = 28, 28, 1\n",
        "  num_filters = 32\n",
        "  latent_dim = 16\n",
        "  num_conv_layers = 3\n",
        "  n_meta_test_episodes = 1000\n",
        "\n",
        "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    # call DataGenerator with k_shot+n_query samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)\n",
        "\n",
        "  exp_string = '.n_way_'+str(n_way) + '.k_shot_' + str(k_shot)  + '.n_query_' + str(n_query) + '.n_meta_test_way_' + str(n_meta_test_way) + '.k_meta_test_shot_' + str(k_meta_test_shot) + '.n_meta_test_query_' + str(n_meta_test_query)\n",
        "\n",
        "  val_accs = OrderedDict()\n",
        "  for ep in range(n_epochs):\n",
        "    for epi in range(n_episodes):\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "\n",
        "      # sample a batch of training data and partition it into\n",
        "      # support and query sets\n",
        "\n",
        "      image_batches, label_batches = data_generator.sample_batch('meta_train', 1, shuffle=False)\n",
        "      support = image_batches[:, :, :k_shot, :].reshape(n_way, k_shot, im_height, im_width, channels)\n",
        "      query = image_batches[:, :, k_shot:, :].reshape(n_way, n_query, im_height, im_width, channels)\n",
        "      labels = label_batches[:, :, k_shot:, :].reshape(n_way, n_query, n_way)\n",
        "\n",
        "      #############################\n",
        "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels)\n",
        "      if (epi+1) % 50 == 0:\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "\n",
        "        # sample a batch of validation data and partition it into\n",
        "        # support and query sets\n",
        "\n",
        "        image_batches, label_batches = data_generator.sample_batch('meta_val', 1, shuffle=False)\n",
        "        support = image_batches[:, :, :k_shot, :].reshape(n_way, k_shot, im_height, im_width, channels)\n",
        "        query = image_batches[:, :, k_shot:, :].reshape(n_way, n_query, im_height, im_width, channels)\n",
        "        labels = label_batches[:, :, k_shot:, :].reshape(n_way, n_query, n_way)\n",
        "\n",
        "        #############################\n",
        "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "        print('[epoch {}/{}, episode {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
        "                                                                    n_epochs,\n",
        "                                                                    epi+1,\n",
        "                                                                    n_episodes,\n",
        "                                                                    ls,\n",
        "                                                                    ac,\n",
        "                                                                    val_ls,\n",
        "                                                                    val_ac))\n",
        "        \n",
        "        val_accs[(ep * n_episodes) + (epi+1)] = val_ac.numpy()\n",
        "\n",
        "  log_file = logdir + 'meta_train' + exp_string + '.pkl'\n",
        "  with open(log_file, 'wb') as f:\n",
        "    pickle.dump(val_accs, f)\n",
        "\n",
        "  print('Testing...')\n",
        "  meta_test_accuracies = []\n",
        "  for epi in range(n_meta_test_episodes):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # support and query sets\n",
        "\n",
        "    image_batches, label_batches = data_generator.sample_batch('meta_test', 1, shuffle=False)\n",
        "    support = image_batches[:, :, :k_meta_test_shot, :].reshape(n_meta_test_way, k_meta_test_shot, im_height, im_width, channels)\n",
        "    query = image_batches[:, :, k_meta_test_shot:, :].reshape(n_meta_test_way, n_meta_test_query, im_height, im_width, channels)\n",
        "    labels = label_batches[:, :, k_meta_test_shot:, :].reshape(n_meta_test_way, n_meta_test_query, n_meta_test_way)\n",
        "\n",
        "    #############################\n",
        "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "    meta_test_accuracies.append(ac)\n",
        "    if (epi+1) % 50 == 0:\n",
        "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
        "  avg_acc = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
        "\n",
        "  meta_test_results = {'meta_test_acc': avg_acc, 'meta_test_acc_std': stds, \n",
        "                       'num_meta_episodes':n_meta_test_episodes}\n",
        "  log_file = logdir + 'meta_test' + exp_string + '.pkl'\n",
        "  with open(log_file, 'wb') as f:\n",
        "    pickle.dump(meta_test_results, f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Tv12fbTQqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673feefb-1e8e-415d-9420-037f9af70c3c"
      },
      "source": [
        "logdir = '/content/drive/MyDrive/CS330-MetaLearning/Hw2/proto_nets/'\n",
        "run_protonet('./omniglot_resized/', logdir = logdir, n_way=5, k_shot=1, n_query=5, n_meta_test_way=5, k_meta_test_shot=4, n_meta_test_query=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1/20, episode 50/100] => meta-training loss: 1.01601, meta-training acc: 0.56000, meta-val loss: 1.16847, meta-val acc: 0.72000\n",
            "[epoch 1/20, episode 100/100] => meta-training loss: 0.77620, meta-training acc: 0.72000, meta-val loss: 0.90558, meta-val acc: 0.80000\n",
            "[epoch 2/20, episode 50/100] => meta-training loss: 0.84824, meta-training acc: 0.72000, meta-val loss: 0.85052, meta-val acc: 0.64000\n",
            "[epoch 2/20, episode 100/100] => meta-training loss: 0.75886, meta-training acc: 0.68000, meta-val loss: 0.60340, meta-val acc: 0.72000\n",
            "[epoch 3/20, episode 50/100] => meta-training loss: 0.66516, meta-training acc: 0.76000, meta-val loss: 1.18981, meta-val acc: 0.40000\n",
            "[epoch 3/20, episode 100/100] => meta-training loss: 0.40272, meta-training acc: 0.96000, meta-val loss: 0.82476, meta-val acc: 0.72000\n",
            "[epoch 4/20, episode 50/100] => meta-training loss: 0.47382, meta-training acc: 0.80000, meta-val loss: 0.64928, meta-val acc: 0.84000\n",
            "[epoch 4/20, episode 100/100] => meta-training loss: 0.81572, meta-training acc: 0.64000, meta-val loss: 0.36726, meta-val acc: 0.88000\n",
            "[epoch 5/20, episode 50/100] => meta-training loss: 0.22598, meta-training acc: 0.92000, meta-val loss: 1.04077, meta-val acc: 0.48000\n",
            "[epoch 5/20, episode 100/100] => meta-training loss: 0.23263, meta-training acc: 0.92000, meta-val loss: 0.46735, meta-val acc: 0.88000\n",
            "[epoch 6/20, episode 50/100] => meta-training loss: 0.45720, meta-training acc: 0.84000, meta-val loss: 0.37715, meta-val acc: 0.88000\n",
            "[epoch 6/20, episode 100/100] => meta-training loss: 0.41536, meta-training acc: 0.84000, meta-val loss: 0.46365, meta-val acc: 0.84000\n",
            "[epoch 7/20, episode 50/100] => meta-training loss: 0.75345, meta-training acc: 0.76000, meta-val loss: 0.17722, meta-val acc: 0.88000\n",
            "[epoch 7/20, episode 100/100] => meta-training loss: 0.50993, meta-training acc: 0.76000, meta-val loss: 0.46396, meta-val acc: 0.80000\n",
            "[epoch 8/20, episode 50/100] => meta-training loss: 0.16549, meta-training acc: 1.00000, meta-val loss: 0.67211, meta-val acc: 0.72000\n",
            "[epoch 8/20, episode 100/100] => meta-training loss: 1.50448, meta-training acc: 0.60000, meta-val loss: 0.59809, meta-val acc: 0.76000\n",
            "[epoch 9/20, episode 50/100] => meta-training loss: 0.32299, meta-training acc: 0.96000, meta-val loss: 0.29621, meta-val acc: 0.84000\n",
            "[epoch 9/20, episode 100/100] => meta-training loss: 0.58510, meta-training acc: 0.76000, meta-val loss: 0.42547, meta-val acc: 0.80000\n",
            "[epoch 10/20, episode 50/100] => meta-training loss: 0.13539, meta-training acc: 0.92000, meta-val loss: 0.30906, meta-val acc: 0.80000\n",
            "[epoch 10/20, episode 100/100] => meta-training loss: 0.17041, meta-training acc: 0.92000, meta-val loss: 0.17219, meta-val acc: 0.92000\n",
            "[epoch 11/20, episode 50/100] => meta-training loss: 0.77916, meta-training acc: 0.68000, meta-val loss: 0.22272, meta-val acc: 0.96000\n",
            "[epoch 11/20, episode 100/100] => meta-training loss: 0.30387, meta-training acc: 0.88000, meta-val loss: 0.51844, meta-val acc: 0.84000\n",
            "[epoch 12/20, episode 50/100] => meta-training loss: 0.04946, meta-training acc: 1.00000, meta-val loss: 0.08628, meta-val acc: 1.00000\n",
            "[epoch 12/20, episode 100/100] => meta-training loss: 0.25511, meta-training acc: 0.92000, meta-val loss: 0.09025, meta-val acc: 0.96000\n",
            "[epoch 13/20, episode 50/100] => meta-training loss: 0.19021, meta-training acc: 0.92000, meta-val loss: 0.15971, meta-val acc: 0.92000\n",
            "[epoch 13/20, episode 100/100] => meta-training loss: 0.17336, meta-training acc: 0.96000, meta-val loss: 0.05040, meta-val acc: 1.00000\n",
            "[epoch 14/20, episode 50/100] => meta-training loss: 0.57694, meta-training acc: 0.88000, meta-val loss: 0.11645, meta-val acc: 0.96000\n",
            "[epoch 14/20, episode 100/100] => meta-training loss: 0.11407, meta-training acc: 0.96000, meta-val loss: 0.59831, meta-val acc: 0.80000\n",
            "[epoch 15/20, episode 50/100] => meta-training loss: 0.11475, meta-training acc: 1.00000, meta-val loss: 0.06079, meta-val acc: 1.00000\n",
            "[epoch 15/20, episode 100/100] => meta-training loss: 0.20691, meta-training acc: 0.92000, meta-val loss: 0.00393, meta-val acc: 1.00000\n",
            "[epoch 16/20, episode 50/100] => meta-training loss: 0.04034, meta-training acc: 1.00000, meta-val loss: 0.27885, meta-val acc: 0.92000\n",
            "[epoch 16/20, episode 100/100] => meta-training loss: 0.37442, meta-training acc: 0.88000, meta-val loss: 0.06980, meta-val acc: 1.00000\n",
            "[epoch 17/20, episode 50/100] => meta-training loss: 0.84788, meta-training acc: 0.72000, meta-val loss: 0.18066, meta-val acc: 0.92000\n",
            "[epoch 17/20, episode 100/100] => meta-training loss: 0.42678, meta-training acc: 0.84000, meta-val loss: 0.14227, meta-val acc: 1.00000\n",
            "[epoch 18/20, episode 50/100] => meta-training loss: 0.21266, meta-training acc: 0.92000, meta-val loss: 0.07000, meta-val acc: 1.00000\n",
            "[epoch 18/20, episode 100/100] => meta-training loss: 0.50465, meta-training acc: 0.84000, meta-val loss: 0.07134, meta-val acc: 1.00000\n",
            "[epoch 19/20, episode 50/100] => meta-training loss: 0.63801, meta-training acc: 0.80000, meta-val loss: 0.08415, meta-val acc: 0.96000\n",
            "[epoch 19/20, episode 100/100] => meta-training loss: 0.23102, meta-training acc: 1.00000, meta-val loss: 0.12503, meta-val acc: 0.96000\n",
            "[epoch 20/20, episode 50/100] => meta-training loss: 0.10889, meta-training acc: 0.96000, meta-val loss: 0.13673, meta-val acc: 1.00000\n",
            "[epoch 20/20, episode 100/100] => meta-training loss: 0.55000, meta-training acc: 0.76000, meta-val loss: 0.41812, meta-val acc: 0.76000\n",
            "Testing...\n",
            "[meta-test episode 50/1000] => loss: 0.19695, acc: 0.90000\n",
            "[meta-test episode 100/1000] => loss: 0.11299, acc: 1.00000\n",
            "[meta-test episode 150/1000] => loss: 0.07485, acc: 0.95000\n",
            "[meta-test episode 200/1000] => loss: 0.19080, acc: 0.90000\n",
            "[meta-test episode 250/1000] => loss: 0.02056, acc: 1.00000\n",
            "[meta-test episode 300/1000] => loss: 0.13093, acc: 1.00000\n",
            "[meta-test episode 350/1000] => loss: 0.20710, acc: 1.00000\n",
            "[meta-test episode 400/1000] => loss: 0.07208, acc: 1.00000\n",
            "[meta-test episode 450/1000] => loss: 0.11456, acc: 0.95000\n",
            "[meta-test episode 500/1000] => loss: 0.49686, acc: 0.90000\n",
            "[meta-test episode 550/1000] => loss: 0.10677, acc: 1.00000\n",
            "[meta-test episode 600/1000] => loss: 0.11619, acc: 1.00000\n",
            "[meta-test episode 650/1000] => loss: 0.03938, acc: 1.00000\n",
            "[meta-test episode 700/1000] => loss: 0.11315, acc: 1.00000\n",
            "[meta-test episode 750/1000] => loss: 0.07231, acc: 1.00000\n",
            "[meta-test episode 800/1000] => loss: 0.21688, acc: 0.90000\n",
            "[meta-test episode 850/1000] => loss: 0.01722, acc: 1.00000\n",
            "[meta-test episode 900/1000] => loss: 0.02120, acc: 1.00000\n",
            "[meta-test episode 950/1000] => loss: 0.67784, acc: 0.85000\n",
            "[meta-test episode 1000/1000] => loss: 0.25717, acc: 0.95000\n",
            "Average Meta-Test Accuracy: 0.96595, Meta-Test Accuracy Std: 0.04726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQa6xkezbfOf",
        "outputId": "81bbb6ee-2b50-4371-de7e-c26e5288cca4"
      },
      "source": [
        "log_file = logdir + 'meta_train.n_way_5.k_shot_1.n_query_5.n_meta_test_way_5.k_meta_test_shot_4.n_meta_test_query_4' + '.pkl'\n",
        "with open(log_file, 'rb') as f:\n",
        "  logs = pickle.load(f)\n",
        "\n",
        "print(dict(logs))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{50: 0.72, 100: 0.8, 150: 0.64, 200: 0.72, 250: 0.4, 300: 0.72, 350: 0.84, 400: 0.88, 450: 0.48, 500: 0.88, 550: 0.88, 600: 0.84, 650: 0.88, 700: 0.8, 750: 0.72, 800: 0.76, 850: 0.84, 900: 0.8, 950: 0.8, 1000: 0.92, 1050: 0.96, 1100: 0.84, 1150: 1.0, 1200: 0.96, 1250: 0.92, 1300: 1.0, 1350: 0.96, 1400: 0.8, 1450: 1.0, 1500: 1.0, 1550: 0.92, 1600: 1.0, 1650: 0.92, 1700: 1.0, 1750: 1.0, 1800: 1.0, 1850: 0.96, 1900: 0.96, 1950: 1.0, 2000: 0.76}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}